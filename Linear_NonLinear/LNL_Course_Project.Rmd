---
title: "LNL_Course_Project"
author: "Patrick Kelly"
date: "Tuesday, March 17, 2015"
output: pdf_document
---

**Course Assignment. Part 1**
---

**1. Problem Description**

The business analytics group of a company is asked to investigate causes of malfunctions in technological process of one of the manufacturing plants that result in significantly increased cost to the end product of the business.
One of suspected reasons for malfunctions is deviation of temperature during the process from optimal levels. The sample in the provided file contains times of malfunctions in seconds since the start of measurement and minute records of temperature.

**2. Data**

The file `MScA_LinearNonLinear_CourseProject.csv` contains time stamps of events expressed in seconds.

Read and prepare the data.

```{r}
Course.Project.Data<-read.csv(file="C:/Users/Patrick/Documents/R/UChicago/Linear_NonLinear/MScA_LinearNonLinear_MalfunctionData.csv")

Course.Project.Data<-as.data.frame(Course.Project.Data)
Course.Project.Data[1:20,]
```

**3. Create Counting Process, Explore Cumulative Intensity**

Counting Process is a step function that jumps by 1 at every moment of new event.

```{r}
Counting.Process<-as.data.frame(cbind(Time=Course.Project.Data$Time,Count=1:length(Course.Project.Data$Time)))

Counting.Process[1:20,]

plot(Counting.Process$Time,Counting.Process$Count,type="s")
```

**3.1 Explore cumulative intensity of the process**

```{r}
plot(Counting.Process$Time,Counting.Process$Count/Counting.Process$Time,
     type="l",ylab="Cumulative Intensity")
abline(h=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)])
abline(h=mean(Counting.Process$Count/Counting.Process$Time))

#check intensity
c(Last.Intensity=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)],
  Mean.Intensity=mean(Counting.Process$Count/Counting.Process$Time))
```

**4. Check for overdispersion**

In order to do that create one-minute counts.

```{r}
#Make 60 second windows of time -- determine the observed counts in each minute duration

# Event.Counts <- hist(ceiling(Counting.Process$Time/60), breaks=250)$counts
ecounts <- table(ceiling(Counting.Process$Time/60))
ecounts_2 <- data.frame(cbind(as.numeric(names(ecounts)),as.numeric(ecounts)))
colnames(ecounts_2) <- c("MINUTES","COUNTS")
ecounts_3 <- data.frame(MINUTES=c(1:250))
Event.Counts <- merge(x=ecounts_2, y=ecounts_3, by="MINUTES", all = TRUE)
Event.Counts$COUNTS[is.na(Event.Counts$COUNTS)] <- 0

barplot(Event.Counts$COUNTS)
```

**4.1 Methods for Testing Overdispersion**

4.1.1 A quick and rough method

Look at the output of `glm()` and compare the residual deviance with the number of degrees of freedom.
If the assumed model is correct deviance is asymptotically distributed as Chi-squared (X2) with degrees of freedom n???k where n is the number of observations and k is the number of parameters.
For Chi-squared distribution X2 distribution the mean is the number of degrees of freedom n???k.
If the residual deviance returned by `glm()` is greated than n???k then it might be a sign of overdispersion.

Test the method on simulated Poisson data.

Note: Deviance has chisquared distribution (mean is the df)

```{r}
Test.Deviance.Overdispersion.Poisson<-function(Sample.Size,Parameter.Lambda){
  my.Sample<-rpois(Sample.Size,Parameter.Lambda)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 
Test.Deviance.Overdispersion.Poisson(100,1)

sum(replicate(1000,Test.Deviance.Overdispersion.Poisson(100,1)))
#sum(replicate(1000,Test.Deviance.Overdispersion.Poisson(150,1)))
#sum(replicate(1000,Test.Deviance.Overdispersion.Poisson(200,1)))

exp(glm(rpois(1000,2)~1,family=poisson)$coeff)
```

Perform the same test on negative binomial data

```{r}
Test.Deviance.Overdispersion.NBinom<-function(Sample.Size,Parameter.prob){
  my.Sample<-rnbinom(Sample.Size,2,Parameter.prob)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 

sum(replicate(1000,Test.Deviance.Overdispersion.NBinom(100,.2)))
```

Now apply the test to the one-minute event counts.

```{r}
GLM.model<-glm(Event.Counts$COUNTS~1,family=poisson)
GLM.model
```

Do you see signs of overdispersion?

Yes, we do observe signs overdisperesion because the null deviance (1798) is considerably higher than the degrees of freedom (248). When this ratio is > 1, as it is for this data, it indicates overdispersion.

**4.1.2 Regression test by Cameron-Trivedi**

The test implemented in AER is described in Cameron, A.C. and Trivedi, P.K. (1990). Regression-based Tests for Overdispersion in the Poisson Model. Journal of Econometrics, 46, 347-364.

In a Poisson model, the mean is E(Y)=lambda and the variance is V(Y)=lambda as well.
They are equal. The test has a null hypothesis c=0 where Var(Y)=lambda+c * f(lambda), c<0 means underdispersion and c>0 means overdispersion.
The function f(.) is some monotonic function (linear (default) or quadratic).
The test statistic used is a t statistic which is asymptotically standard normal under the null.

```{r}
library(AER)

mysample<-rpois(100,1)
model<-glm(mysample~1,family=poisson)

mysample2<-rnbinom(100,2,0.2)
model2<-glm(mysample2~1,family=poisson)

#Disp.Test
dispersiontest(model)
dispersiontest(model2)
```

**4.1.3 Test against Negative Binomial Distribution**

The null hypothesis of this test is that the distribution is Poisson as particular case of Negative binomial against Negative Binomial.

The references are:
A. Colin Cameron and Pravin K. Trivedi (1998) Regression analysis of count data. New York: Cambridge University Press.

Lawless, J. F. (1987) Negative Binomial and Mixed Poisson Regressions. The Canadian Journal of Statistics. 15:209-225.

Required packages are MASS (to create a negative binomial object with glm.nb) and pscl the test function is odTest.

```{r}
library(MASS)
library(pscl)

#fit the negative binomial and test which negative binomial it is likely to be --- if it is general negative binomial then it is not poisson, but we may find that it is poisson that fits the best ... 

#small p-value = reject that it is poisson

# Test.Deviance.Overdispersion.Poisson2<-function(samplesize,param_lambda){
#   p_sample<-rpois(samplesize,param_lambda)
#   p_sample<-data.frame(p_sample_var = p_sample)
#   nbmodel<-glm.nb(p_sample_var~1,data=p_sample)
#   odtest_out <- odTest(nbmodel)
#   return(odtest_out)
# }
# 
# Test.Deviance.Overdispersion.Poisson2(100,5)

  p_sample<-rpois(100,1)
  nbmodel<-glm.nb(p_sample~1)
  odtest_out <- odTest(nbmodel)

# Test.Deviance.Overdispersion.NBinom2<-function(samplesize,param_prob){
#   rnsample<-rnbinom(samplesize,2,param_prob)
#   nbmodel<-glm.nb(rnsample~1)
#   odTest(nbmodel)
# } 
# 
# Test.Deviance.Overdispersion.NBinom2(100,.2)

  rnsample<-rnbinom(100,2,0.2)
  nbmodel<-glm.nb(rnsample~1)
  odTest(nbmodel)

```

**5. Find the distribution of Poisson intensity**

5.1. Kolmlgorov-Smirnov test

Kolmogorov-Smirnov test is used to test hypotheses of equivalence between two empirical distributions or equivalence between one empirical distribution and one theoretical distribution.

```{r}
library(lattice)
library(latticeExtra)

sample1=rnorm(100)
sample2=rnorm(100,1,2)
Cum.Distr.Functions <- data.frame(sample1,sample2)
ecdfplot(~ sample1 + sample2, data=Cum.Distr.Functions, auto.key=list(space='right'))
```

Check equivalence of empirical distributions for the two samples.

```{r}
ks.test(sample1,sample2)
```

Check eqiovalence of empirical distribution of `sample1` and theoretical distribution `Norm(0,1)`.

```{r}
ks.test(sample1,"pnorm",mean=0,sd=1)
```

Check eqiovalence of empirical distribution of `sample2` and theoretical distribution `Norm(0,1)`.

```{r}
ks.test(sample2,"pnorm",mean=0,sd=1)
```

5.2. Check the distribution for the entire period

Apply Kolmogorov-Smirnov test to `Counting.Process$Time` and theoretical exponential distribution with parameter equal to average intensity.

Hint: the empirical distribution should be estimated for time intervals between malfunctions.

Plot empirical cumulative distribution function for time intervals between malfunctions.

```{r}
#check ks test
fit1 <- fitdistr(Counting.Process$Time, "exponential") 
ks.test(Counting.Process$Time, "pexp", fit1$estimate)
#ks.test(Counting.Process$Time, "pexp", (1/mean(Counting.Process$Time)))

#fit2 <- fitdistr(time_differences, "exponential") 
#ks.test(time_differences, "pexp", fit2$estimate)
# ks.test(time_differences, "pexp", rate=(1/mean(time_differences)))

#create vector of differences
time_differences <- diff(Counting.Process$Time)

#check the culm dist plot
ecdfplot(~time_differences)

```


**5.3. Check distribution of one-minute periods**

Use at least 5 different candidates for distribution of Poisson intensity of malfunctions.

Find one-minute intensities.

```{r}
#create a cumulative counts
Event.Counts <- cbind.data.frame(Event.Counts, CUM.COUNTS = cumsum(Event.Counts$COUNTS))

#calculate intensities
Event.Intensities <- Event.Counts$COUNTS / 60
hist(Event.Intensities)#, breaks=20)
plot(sort(Event.Intensities))
plot(Event.Intensities)
plot(Event.Intensities, type = "l")
```

Fit 5 different distributions to `Event.Intensities` using `fitdistr()` from MASS.

Recommendation: start with fitting normal and exponential distributions first.

```{r}
#fit normal distribution
Fitting.Normal <- fitdistr(Event.Intensities, "normal")
Fitting.Normal

#fit exponential distribution
Fitting.Exponential <- fitdistr(Event.Intensities, "exponential")
Fitting.Exponential
```

Test the fitted distributions with Kolmogorov-Smirnov test.

```{r}
KS.Normal <- ks.test(Event.Intensities,"pnorm",
                     mean=Fitting.Normal$estimate[1],sd=Fitting.Normal$estimate[2])

c(KS.Normal$statistic,P.Value=KS.Normal$p.value)

KS.Exp <- ks.test(Event.Intensities,"pexp",rate=Fitting.Exponential$estimate)

c(KS.Exp$statistic,P.Value=KS.Exp$p.value)
```

Try to fit gamma distribution directly

Estimate parameters of gamma distribution using method of moments.

```{r}
xbar <- mean(Event.Intensities)
n <- length(Event.Intensities)
v <- var(Event.Intensities)* (n-1)/n

Moments.Rate <- xbar/v
Moments.Shape <- (xbar)^2/v

```

Check gamma distribution with these parameters as a theoretical distribution using Kolmogorov-Smirnov test.

```{r}
KS.Test.Moments <- ks.test(Event.Intensities,"pgamma",shape=Moments.Shape,rate=Moments.Rate)
KS.Test.Moments
```

Find at least 2 more candidates and test them by Kolmogorov-Smirnov.

```{r}
#fit poisson dist
Fitting.Poisson <- suppressWarnings(fitdistr(Event.Intensities,"Poisson"))
Fitting.Poisson

KS.Candidate.4 <- ks.test(Event.Intensities,"ppois",lambda=Fitting.Poisson$estimate[1])
KS.Candidate.4

#fit geometric dist 
Fitting.Geom <- fitdistr(Event.Intensities,"geometric")
Fitting.Geom

KS.Candidate.5 <- ks.test(Event.Intensities,"pgeom", prob = Fitting.Geom$estimate[1])
KS.Candidate.5
```

Collect all estimated distributions together and make your choice.

```{r}
rbind(KS.Moments=c(KS.Test.Moments$statistic,P.Value=KS.Test.Moments$p.value),
      KS.Candidate.4=c(KS.Candidate.4$statistic,P.Value=KS.Candidate.4$p.value),
      KS.Candidate.5=c(KS.Candidate.5$statistic,P.Value=KS.Candidate.5$p.value),
      KS.Exp=c(KS.Exp$statistic,P.Value=KS.Exp$p.value),
      KS.Normal=c(KS.Normal$statistic,KS.Normal$p.value))
```

**What distribution for the one-minute intensity of malfunctions do you choose?**

The Gamma distribution. This decision is based on the fits and KS tests above - observe that the gamma fit produces the only passing pvalue from the above test, indicating its gamma distribution fit

**What is the resulting distribution of the counts of one-minute malfunctions for your choice?**

The resulting distribution of the counts of one-minute malfunctions is a negative-binomial distribution.



**Course Assignment. Part 2**
---

**Explore possible types of dependence between one-minute counts and temperature.**

Read the data and create a data frame with one-minute breaks counts and temperature measurements.

Create data frame with necessary data.

```{r}
Part2.Data<-read.csv(file="C:/Users/Patrick/Documents/R/UChicago/Linear_NonLinear/MScA_LinearNonLinear_CourseProject_Part2.csv")
Part2.Data<-as.data.frame(cbind(Part2.Data,Part2.Data[,2]/60))
colnames(Part2.Data)<-c("Times","Counts","Temperatures","Intensities")
head(Part2.Data)
```

Visualize the data.

```{r}
plot(Part2.Data$Temperatures,Part2.Data$Intensities)
```

**Interpret the plot. What type of relationship you observe?**

The plot above defintiely suggest a positive relationship between the temperature and intensity.


Analyze empirical copula.

```{r}
plot(rank(Part2.Data$Temperatures),rank(Part2.Data$Intensities))
```

**What type of dependency you see in the empirical copula?**

The empirical copula follows the findings from above that there is a positive relationship in the data.


What is the distribution of temperatures?
Load package `MASS` to estimate distributions

```{r}
library(MASS)
```

Observe the histogram

```{r}
hist(Part2.Data$Temperatures)
```

Estimate and test normal distribution.

```{r}
Fit.Temp.Normal <- fitdistr(Part2.Data$Temperatures,"normal")
Fit.Temp.Normal

ks.test(Part2.Data$Temperatures,"pnorm",
        mean=Fit.Temp.Normal$estimate[1],sd=Fit.Temp.Normal$estimate[2])
```

**Fit a copula**

Select a parametric copula appropriate for this case.

Fit the copula and use it for simulation of rare events.

```{r}
library(copula)

# cop1 <- normalCopula()
cop1 <- gumbelCopula()
p1 <- pobs(na.omit(Part2.Data[,3:4]), ties.method = "average")

Copula.Fit <- fitCopula(cop1,p1)

Copula.Fit
```
```{r,echo=FALSE}
#you fit copula for simulation ... without the copula the rare occurance does not allow for a lot of data in a simulation
#use clayton copula for lower left occurance
#use gumbal for upper right

#we then get two distributions, gamma and normal

#normalize so that copula values are between 0 and 1

#we will be fitting a negative binomial model ... we found intensity has gamma distribution

#we tried negative

#when you seperate the tail events, they may be better fit by poisson because they are rare events
#almost all processes can be turned into poisson through "thinning" 
  #ex: for each observation you randomly accept / observe it with a probability of .5

#since theta is greater than std err, what does that mean? variance is smaller
  #how does it affect the model? 
      #theta relates to overdispersion.... overdispersion gets lower when theta is larger ... 
      #closer to poisson ... "I recommend to go and try and fit poisson"
          #"so we did get poisson when we did thinning"
```

Run longer simulation to observe more tail events Use the parameter estimates of the gamma distribution fitted to intensities.

```{r}
Gamma.Rate<-8.132
Gamma.Shape<-1.656
```

and the parameter estimates of the normal distribution fitted to temperatures.

```{r}
Fit.Temp.Normal$estimate
```

Simulate 5000 pairs of intensities and temperatures using the estimated copula.

```{r}
Defined.Copula <- gumbelCopula(param = 1.8775, dim = 2)

Simulated.Copula<-rCopula(5000,Defined.Copula)
Simulated.Intensities<-qgamma(Simulated.Copula[,1],shape=Gamma.Shape,scale=1/Gamma.Rate)
Simulated.Temperature<-qnorm(Simulated.Copula[,2],Fit.Temp.Normal$estimate[1],Fit.Temp.Normal$estimate[2])
```

Plot the simulated variables and their empirical copula.

```{r}
plot(Simulated.Temperature,Simulated.Intensities)
plot(rank(Simulated.Intensities),rank(Simulated.Temperature))
```

Now we can use the simulated data to analyze the tail dependency.
Select the simulated pairs wit intensity greater than 0.5 and temperature greater than 110.
Use these data to fit negative binomial regression.

Use the initial sample of intensities and temperatures to fit the negative binomial regression for more regular ranges of intensity and temperature.

First, run fit the model to the sample.

```{r}
m1 <- glm.nb(formula = Counts ~ Temperatures, data = Part2.Data)
summary(m1)
```

Create the simulated sample for tail events.

```{r}
Simulated.Tails<-as.data.frame(
  cbind(round(Simulated.Intensities[(Simulated.Temperature>110)&(Simulated.Intensities>.5)]*60),
        Simulated.Temperature[(Simulated.Temperature>110)&(Simulated.Intensities>.5)]))
colnames(Simulated.Tails)<-c("Counts","Temperatures")

plot(Simulated.Tails$Temperatures,Simulated.Tails$Counts)

#model for the sample tail events
m2 <- glm.nb(formula = Counts ~ Temperatures, data = Simulated.Tails)
summary(m2)
```

Compare the summaries of the two models. Note that the parameter ?? estimated by glm.nb() defines the variance of the model as ??+??2/??, where ?? is the mean. In other words, ?? defines overdispersion.

**What do the fitted parameters ?? tell you about both models**

The higher theta values correspond with lower dispersion (or less over-dispersion) within the data. Therefore higher ?? values indicate that the distribution is closer to a Poisson distribution (rather than a neg. binomial). Since we see a much higher ?? value in the simulated tails and a low ?? value in the model of the entire data, we can conclude that the simulated tails data has less over-dispersion. This also intuitively makes sense given the model fit, as the extreme values are more likely to have a more predictable increase in malfunctions - in comparison to the variablility of occurances when the temperature is within a normal / non-extreme range. 

**Is there an alternative model that you would try to fit to the simulated tail data?**

Based on the above explanation, I would try to fit a Poisson model as an alternative for the simulated tail data.

**What do both models tell you about the relationships between the temperature and the counts?**

Both models have similiar coefficients that indicate that there is a significant positive relationship between the temperature and malfunction counts. 